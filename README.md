AI-Based FPGA Accelerator for Electric Vehiclesâ€™ Battery Management System

Project Status: Ongoing

Team Members:

    MANI BARATHI P

    VEL GNANA SATISH M

    RAJIV PRASAAD

ğŸ“˜ Project Overview

This project focuses on developing a high-accuracy, low-latency State-of-Charge (SoC) prediction engine for Lithium-ion batteries used in Electric Vehicles (EVs).
The work integrates AI-based SoC estimation (LSTM) with FPGA-based hardware acceleration, enabling real-time deployment inside Battery Management Systems (BMS).

The project replicates and extends the methodology used in the research paper:
â¡ï¸ â€œAn FPGA-Based LSTM Accelerator for SoC Prediction in Lithium-Ion Batteriesâ€ (reference paper used for Phase-1).

ğŸš€ Motivation

Traditional SoC estimation techniquesâ€”Coulomb counting, Kalman filters, and equivalent circuit modelingâ€”are:

Model-dependent

Limited by linearity assumptions

Sensitive to noise, aging, and temperature variation

AI approaches overcome these limitations by learning nonlinear electrochemical dynamics directly from data.
However, deploying neural networks in embedded BMS systems requires low power, low latency, and real-time inferenceâ€”which leads to FPGA acceleration.

ğŸ§  Phase 1: AI Model Development (Completed)
ğŸ“Œ Dataset Used

Panasonic 18650PF Li-ion Battery Cycling Dataset
Provided by Dr. Phillip Kollmeyer (University of Wisconsinâ€“Madison).

Cycles Used: 1â€“4 discharge cycles at 25Â°C

ğŸ“Š Features (Inputs)

Voltage

Current

Battery Temperature

Timestamp

ğŸ¯ Target

State of Charge (SoC)

ğŸ›  Preprocessing Steps

Loaded .mat files

Removed NaN & outliers

MinMax normalization

80/20 trainâ€“test split

Sliding window generation (look-back 60)

ğŸ“ˆ Phase 1: LSTM Model Training (Completed)
ğŸ§© LSTM Architecture

Look-back window: 60

Hidden units: 5

Optimizer: Adam, learning rate 0.1

Epochs: 100

Batch size: 60

ğŸ“‰ Achieved Accuracy

Training RMSE: 0.3438

Validation RMSE: 0.3681

Model training implemented and validated using Google Colab.
Colab notebook link:
ğŸ”— https://colab.research.google.com/drive/1y1bPJLSouUYYb7cqpMXGproDoee-XFyw?usp=sharing

âš™ï¸ Phase 1: Hardware Translation (Completed)
ğŸ“¥ Weight Extraction

Performed using:

model.get_weights()


Weights & biases exported into C++ header files as constant arrays.

ğŸ”§ C++ LSTM Inference Engine

Implements LSTM forward pass

Includes matrix multiplications, activations, cell updates

Written using HLS-synthesizable constructs

ğŸ— High-Level Synthesis

Using Xilinx Vitis HLS, the C++ model was:

Simulated

Optimized with HLS pragmas

Synthesized into RTL (Verilog/VHDL)

This RTL is ready for integration into:

Xilinx Zynq SoC

PYNQ-Z2 board

Any FPGA-based BMS prototype

ğŸ§© Current Phase (Ongoing Work)

RTL verification

FPGA resource utilization analysis

Latency & power benchmarking

Preparing system-level integration for real BMS deployment

ğŸ¯ Final Goal

To build a fully functional FPGA-based SoC estimation subsystem capable of:

Real-time inference

Low latency

Low power consumption

High SoC prediction accuracy

Compatibility with real EV Battery Management Systems

ğŸ“‚ Repository Structure 
â”œâ”€â”€ dataset/                 # Panasonic data files (.mat)
â”œâ”€â”€ notebooks/               # Colab notebooks
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ python_model/        # LSTM training code
â”‚   â”œâ”€â”€ hls_cpp/             # C++ inference code for Vitis HLS
â”‚   â”œâ”€â”€ headers/             # Exported weight files (.h)
â”‚   â””â”€â”€ rtl/                 # Generated RTL (Verilog/VHDL)
â”œâ”€â”€ docs/                    # Reports, diagrams, explanations
â””â”€â”€ README.md                # Project documentation